\documentclass{article}
\usepackage{amsmath}

\usepackage{listings}
\usepackage{color}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour}
    }

\lstset{style=mystyle}

\title{Statistical Modeling 2 \\ Exercise 2}

\begin{document}
\maketitle
\section{A simple Gaussian location model}
\subsection*{A}
The joint prior over the mean parameter \(\theta\) and precision parameter \(\omega\) is:
\begin{align*}
p(\theta, \omega) \propto \omega^{(d+1)/2-1} \exp \left\lbrace -\omega\frac{\kappa(\theta-\mu)^2}{2}\right\rbrace \exp \left\lbrace -\omega\frac{\eta}{2} \right\rbrace
\end{align*}
To get the marginal prior, we integrate out the parameter \(\omega\):
\begin{align*}
p(\theta) &\propto \int_0^{\infty} \omega^{(d+1)/2-1} \exp \left\lbrace -\omega\frac{\kappa(\theta-\mu)^2 + \eta}{2}\right\rbrace\\
&\propto \left(\frac{\kappa(\theta-\mu)^2 + \eta}{2}\right)^{-(d+1)/2} \\
&= \left(\frac{\eta}{2} + \frac{\kappa(\theta-\mu)^2}{2}\right)^{-(d+1)/2} \\
&= \left(1 + \frac{\kappa(\theta-\mu)^2}{\eta}\right)^{-(d+1)/2} \left(\frac{\eta}{2}\right)^{-(d+1)/2} \\
&\propto \left(1 + \frac{\kappa(\theta-\mu)^2}{\eta}\right)^{-(d+1)/2}\\
&=\left(1 + \frac{1}{d}\frac{\kappa(\theta-\mu)^2}{\eta}\right)^{-(d+1)/2}\\
\end{align*}

Let \(\nu = d, m = \mu\) and \(s = \sqrt{\eta/\kappa}\), we have a Student t distribution with \(\nu\) degrees of freedom and scale \(s\):
\begin{align*}
p(\theta) \propto \left(1 + \frac{1}{\nu}\frac{(\theta-m)^2}{s^2}\right)^{-(\nu+1)/2}\\
\end{align*}

\subsection*{B}
The sampling model is:
\begin{align*}
(y_i\mid \theta, \omega) \sim N(\theta, 1/\omega)
\end{align*}
where \(y_1, \dots, y_n\) are the datapoints, \(\theta\) is the mean and \(\omega\) is the precision. We have that the likelihood for all the datapoints can be written as:
\begin{align*}
p(\mathbf{y} \mid \theta, \omega) &\propto \prod_{i=1}^n \omega^{1/2} \exp \left\lbrace -\frac{1}{2}\omega (y_i - \theta)^2 \right\rbrace\\
&= \omega^{n/2} \exp \left\lbrace -\frac{1}{2}\omega \sum_{i=1}^n (y_i - \theta)^2 \right\rbrace\\ 
&= \omega^{n/2} \exp \left\lbrace -\frac{1}{2}\omega \left( \sum_{i=1}^n y_i^2 + \sum_{i=1}^n\theta^2 - 2\sum_{i=1}^n y_i\theta\right) \right\rbrace\\ 
&= \omega^{n/2} \exp \left\lbrace -\frac{1}{2}\omega \left( \sum_{i=1}^n y_i^2 + n\theta^2 - 2n\overline{y}\theta + n\overline{y}^2 - n\overline{y}^2 \right) \right\rbrace\\ 
\end{align*}
where \(\overline{y} = \left(\sum_{i=1}^n y_i\right)/n\). Let \(S_y = \sum_{i=1}^n (y_i - \overline{y})^2\), we have:
\begin{align*}
S_y &= \sum_{i=1}^n y_i^2 + n \overline{y}^2 - 2 \sum_{i=1}^n y_i \overline{y}\\
&= \sum_{i=1}^n y_i^2 - n\overline{y}^2
\end{align*}
Therefore, the likelihood is:
\begin{align*}
p(\mathbf{y} \mid \theta, \omega) &= \omega^{n/2} \exp \left\lbrace -\frac{1}{2}\omega \left[ S_y + n(\theta^2 - 2\overline{y}\theta + \overline{y}^2) \right] \right\rbrace\\ 
&= \omega^{n/2} \exp \left\lbrace -\frac{1}{2}\omega \left[ S_y +  n (\overline{y} - \theta)^2\right] \right\rbrace\\ 
\end{align*}

The posterior is proportional to the product of the likelihood and the prior:
\begin{align*}
p(\theta, \omega \mid \mathbf{y}) &\propto \omega^{(d+1)/2 - 1} \exp\left\lbrace - \omega \frac{\kappa(\theta-\mu)^2}{2} \right\rbrace \exp\left\lbrace -\omega \frac{\eta}{2}\right\rbrace\\
&\omega^{n/2} \exp \left\lbrace -\frac{1}{2}\omega \left[ S_y +  n (\overline{y} - \theta)^2\right] \right\rbrace\\ 
&= \omega^{(d+n+1)/2 - 1}\exp\left\lbrace - \omega \frac{\kappa(\theta-\mu)^2 + n(\theta - \overline{y})^2}{2} \right\rbrace \exp\left\lbrace -\omega \frac{\eta + S_y}{2}\right\rbrace\\
\end{align*}

We also have:
\begin{align*}
\kappa(\theta-\mu)^2 + n (\theta - \overline{y})^2 &= \kappa\theta^2 + \kappa\mu^2 - 2\kappa\theta\mu + n\theta^2 + n\overline{y}^2 - 2n\theta\overline{y}\\
&= (\kappa + n)\theta^2 - 2\theta(\kappa\mu + n\overline{y}) + (\kappa\mu^2 + n\overline{y}^2)\\
&=(\kappa+n)\left( \theta^2 - 2 \theta \frac{\kappa\mu + n\overline{y}}{\kappa + n} + \frac{(\kappa\mu + n\overline{y})^2}{(\kappa + n)^2} \right) - \frac{(\kappa\mu + n\overline{y})^2}{\kappa + n} + (\kappa\mu^2 + n\overline{y}^2)\\
&= (\kappa+n)\left( \theta^2 -\frac{\kappa\mu + n\overline{y}}{\kappa + n}\right)^2 - \frac{(\kappa\mu + n\overline{y})^2}{\kappa + n} + (\kappa\mu^2 + n\overline{y}^2)\\
\end{align*}


Therefore, the posterior is:
\begin{align*}
p(\theta, \omega \mid \mathbf{y}) &\propto \omega^{(d^*+1)/2 - 1} \exp\left\lbrace - \omega \frac{\kappa^*(\theta-\mu^*)^2}{2} \right\rbrace \exp\left\lbrace -\omega \frac{\eta^*}{2}\right\rbrace\\
\end{align*}
where:
\begin{align*}
&d^* = d + n\\
&\kappa^* = \kappa + n\\
&\mu^* = \frac{\kappa\mu + n\overline{y}}{\kappa + n}\\
\end{align*}
and
\begin{align*}
\eta^* &= \eta + S_y - \frac{(\kappa\mu + n\overline{y})^2}{\kappa + n} + (\kappa\mu^2 + n\overline{y}^2)\\
&= \eta + S_y + \frac{(\kappa+n)(\kappa\mu^2 + n\overline{y}^2) - \kappa^2\mu^2 - n^2\overline{y}^2 + 2\kappa n\mu \overline{y} }{\kappa + n}\\
&= \eta + S_y + \frac{\kappa n\mu^2 + \kappa n\overline{y}^2 + 2\kappa n\mu \overline{y} }{\kappa + n}\\
&= \eta + S_y + \frac{\kappa n (\mu + \overline{y})^2}{\kappa + n}
\end{align*}

\subsection*{C}
The conditional distribution is:
\begin{align*}
p(\theta \mid \mathbf{y}, \omega) &\propto \exp\left\lbrace - \omega \frac{\kappa^*(\theta-\mu^*)^2}{2} \right\rbrace
\end{align*}
We see that this is a Normal distribution with mean \(\mu^*\) and variance \(1/(\omega\kappa^*)\).

\subsection*{D}
The marginal posterior of \(\omega\) is:
\begin{align*}
p(\omega \mid \mathbf{y}) &= \int_{-\infty}^{\infty} p(\omega, \theta \mid \mathbf{y}) d\theta\\
&\propto \omega^{(d^*+1)/2 - 1}\exp\left\lbrace -\omega \frac{\eta^*}{2}\right\rbrace \int_{-\infty}^{\infty} \exp\left\lbrace - \omega \frac{\kappa^*(\theta-\mu^*)^2}{2} \right\rbrace d\theta\\
&\propto \omega^{(d^*+1)/2 - 1}\exp\left\lbrace -\omega \frac{\eta^*}{2}\right\rbrace \tag{Gaussian integral}
\end{align*}
We see that this marginal is a Gamma distribution with parameter \((d^*/2, \eta^*/2)\).
\subsection*{E}
The marginal posterior of \(\theta\) is:
\begin{align*}
p(\theta \mid \mathbf{y}) &= \int_0^{\infty} p(\theta, \omega \mid \mathbf{y}) d\omega\\
&= \int_0^{\infty} \omega^{(d^*+1)/2-1} \exp \left\lbrace -\omega\frac{\kappa^*(\theta-\mu^*)^2 + \eta^*}{2}\right\rbrace\\
\end{align*}
This is the same integral in part A. By the results in A, we can see that this marginal is a Student t distribution with parameters \(\nu = d^*, m = \mu^*\) and \(s = \sqrt{\eta^*/\kappa^*}\).

\subsection*{F}
FALSE. As \(\kappa\) approaches \(0\), the Normal prior on \(\theta\) approaches a point distribution but the density at that point is infinite. As \(d\) and \(\eta\) approach \(0\), the Gamma prior on \(\omega\) also approach a point distribution with infinite density.

\subsection*{G}
By the results in D and E, we see that when the prior parameters approach \(0\), the posterior parameters are not \(0\) then \(p(\theta \mid \mathbf{y})\) and \(p(\omega \mid \mathbf{y})\) are valid distribution.


\subsection*{H}
The classical frequentest confidence interval for \(\theta\) is:
\begin{align*}
\overline{y} \pm t^*\frac{\sqrt{S_y}}{\sqrt{n(n-1)}}
\end{align*}

%the sampling model approaches a Normal distribution with infinite variance. 
As the prior parameters \(\kappa, d, \eta\) approach \(0\), we have the Bayesian credible interval for \(\theta\) is:
\begin{align*}
m \pm t^*s
\end{align*}
from the results in B and E, we have
\begin{align*}
m = \mu^* = \overline{y}
\end{align*}
and
\begin{align*}
s = \sqrt{\eta^*/\kappa} = \sqrt{\frac{S_y + \overline{y}/n}{n}}
\end{align*}
We see that this is different from the classical confidence interval.
\section{The conjugate Gaussian linear model}
\subsection*{A}
\begin{align*}
p(\beta \mid \mathbf{y}, \omega) &\propto p(\beta \mid \omega) p(\mathbf{y} \mid \beta, \omega)\\
\end{align*}




\end{document}