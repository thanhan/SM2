\documentclass{article}
\usepackage{amsmath}

\title{Statistical Modeling 2 \\ Exercise 1}

\begin{document}
\maketitle

\section{Bayesian inference in simple conjugate families}
\subsection*{A}
\begin{align}
p(w \mid x_1, \ldots, x_N) &\propto p(x_1, \ldots, x_N \mid w) p(w) \tag{Bayes rule} \\
&\propto \prod_{i=1}^N p(x_i|w) w^{a-1}(1-w)^{b-1} \tag{indepence} \\
&\propto w^s (1-w)^{N-s}w^{a-1}(1-w)^{b-1} \tag{let \(s = \sum_{i=1}^N x_i\)} \\
&= w^{s+a-1}(1-w)^{N-s+b-1} \tag*{}\\
& \propto \mathrm{Beta}(s+a, N-s+b) \tag*{}
\end{align}

\subsection*{B}
Let \(f(x_1, x_2) = (y1, y2) = (x_1/(x_1+x_2), x_1+x_2) \), we have:
\begin{align*}
f^{-1}(y1, y2) = (x_1, x_2) = (y_1y_2, y_2 - y_1y_2)
\end{align*}
We then calculate the Jacobian of \(f^{-1}\):
\begin{align*}
\partial x_1 / \partial y_1 &= y_2 \\
\partial x_1 / \partial y_2 &= y_1 \\
\partial x_2 / \partial y_1 &= -y_2 \\
\partial x_2 / \partial y_2 &= 1-y_1\\
\end{align*}
Therefore,
\begin{align*}
|J(f^{-1})| &= \begin{vmatrix}
y_2 & y_1 \\
-y_2 & 1 - y_1
\end{vmatrix} \\
&= y_2(1-y_1) + y_1y_2\\
&= y_2
\end{align*}
Let \(p_X\) be the joint density of \((x_1, x_2)\). We have the joint density of \(y_1\) and \(y_2\):
\begin{align*}
p(y_1, y_2) &= p_X(f^{-1}(y_1,y_2)) |J(f^{-1}(y_1, y_2))| \\
&=\mathrm{Ga}(y_1y_2; a_1, 1) \mathrm{Ga}(y2 - y_1y_2; a_2, 1) y_2\\
&= \frac{(y_1y_2)^{a_1-1} \exp(-y_1y_2)}{\Gamma(a_1)}
\frac{((1-y_1)y_2)^{a_2-1} \exp(y_1y_2-y_2)}{\Gamma(a_2)} y_2\\
&= \frac{y_1^{a_1-1}y_2^{a_1+a_2-1}(1-y_1)^{a_2-1}\exp(-y_2)}{\Gamma(a_1)\Gamma(a_2)}
\end{align*}
The marginals are:
\begin{align*}
p(y_1) &= \int_{0}^{\infty} p(y_1, y_2) dy_2\\
&= \frac{y_1^{a_1-1}(1-y_1)^{a_2-1}}{\Gamma(a_1)\Gamma(a_2)} \int_{0}^{\infty}y_2^{a_1+a_2-1}\exp(-y_2) dy_2\\
&=\frac{y_1^{a_1-1}(1-y_1)^{a_2-1}\Gamma
(a_1+a_2)}{\Gamma(a_1)\Gamma(a_2)}
\end{align*}
and 
\begin{align*}
p(y_2) &= \int_{0}^{\infty} p(y_1, y_2) dy_1\\
&=\frac{y_2^{a_1+a_2-1}\exp(-y_2)}{\Gamma(a_1)\Gamma(a_2)}\int_{0}^{\infty}y_1^{a_1-1}(1-y_1)^{a_2-1}\\
&= \frac{y_2^{a_1+a_2-1}\exp(-y_2)}{\Gamma(a_1)\Gamma(a_2)} \mathrm{Beta}(a_1, a_2)
\end{align*}

\subsection*{C}
The posterior is:
\begin{align*}
p(\theta| x_1, \ldots, x_N) &\propto p(x_1, \ldots, x_N | \theta) p(\theta)\\
&=\prod_{i=1}^N \mbox{N}(x_i; \theta, \sigma^2) \mbox{N}(\theta; m, v)\\
&\propto \prod_{i=1}^N \exp\left( -\frac{(x_i-\theta)^2}{2\sigma^2} \right) \exp\left( -\frac{(\theta - m)^2}{2v} \right)\\
&= \prod_{i=1}^N\exp\left(- \frac{x_i^2-2x_i\theta + \theta^2}{2\sigma^2} \right) \exp\left( -\frac{\theta^2 - 2\theta m + m^2}{2v} \right)\\
&=\exp\left( \frac{-\sum_i x_i^2+2\sum_i x_i\theta - N\theta^2}{2\sigma^2} \right) \exp\left( \frac{-\theta^2 + 2\theta m - m^2}{2v} \right)\\
&= \exp \left( -\theta^2\left(\frac{N}{2\sigma^2} + \frac{1}{2v} \right) + \theta\left(\frac{\sum_ix_i}{\sigma^2} + \frac{m}{v}\right) - \frac{\sum_ix_i^2}{2\sigma^2} - \frac{m^2}{2v} \right)
\end{align*}
We then complete the square by setting the posterior to:

\begin{align*}
&=\exp \left[ -a\left(\theta^2 -2b\theta + b^2\right) \right]\\
&=\exp \left[ -a\left(\theta - b\right)^2 \right]\\
&=\exp \left[ -\frac{(\theta - b)^2}{2(1/(2a))} \right]\\
\end{align*}


We calculate \(a, b\) by matching coefficients:

\begin{align*}
a &= \frac{N}{2\sigma^2} + \frac{1}{2v} = \frac{Nv + \sigma^2}{2\sigma^2v}\\
2ab &= \frac{\sum_ix_i}{\sigma^2} + \frac{m}{v}\\
\implies b &= \frac{v\sum_ix_i + m\sigma^2}{v\sigma^2} \frac{\sigma^2v}{Nv + \sigma^2}\\
&=\frac{v\sum_ix_i + m\sigma^2}{Nv + \sigma^2}
\end{align*}

The posterior is then:
\begin{align*}
&\mbox{N}(b, 1/(2a))\\
=&\mbox{N}\left(\frac{v\sum_ix_i + m\sigma^2}{Nv + \sigma^2}, \frac{\sigma^2v}{Nv + \sigma^2} \right)
\end{align*}

\subsection*{D}
\begin{align*}
p(\omega \mid x_1, \ldots, x_N) &\propto \prod_{i=1}^N p(x_i \mid \theta, \omega) p(\omega)\\
&\propto \prod_{i=1}^N \omega^{1/2} \exp \left[- \frac{\omega}{2}(x_i-\theta)^2 \right] \frac{b^a}{\Gamma(a)}\omega^{a-1}\exp(-b\omega)\\
&\propto \omega^{N/2 + a - 1} \exp\left[ -\omega \left( b + \frac{\sum_i(x_i-\theta)^2}{2} \right) \right]\\
&\propto \mbox{Ga}\left(a+\frac{N}{2}, b + \frac{\sum_i(x_i-\theta)^2}{2} \right)
\end{align*}
We have the posterior of the variance:
\begin{align*}
p(\sigma^2\mid x_1, \ldots, x_N) = \mbox{IG}\left(a+\frac{N}{2}, b + \frac{\sum_i(x_i-\theta)^2}{2} \right)
\end{align*}

\subsection*{E}
The posterior is:
\begin{align*}
p(\theta| x_1, \ldots, x_N) &\propto p(x_1, \ldots, x_N | \theta) p(\theta)\\
&=\prod_{i=1}^N \mbox{N}(x_i; \theta, \sigma_i^2) \mbox{N}(\theta; m, v)\\
&\propto \prod_{i=1}^N \exp\left( -\frac{(x_i-\theta)^2}{2\sigma_i^2} \right) \exp\left( -\frac{(\theta - m)^2}{2v} \right)\\
&= \exp\left( -\sum_{i=1}^n\frac{(x_i-\theta)^2}{2\sigma_i^2}  -\frac{(\theta - m)^2}{2v} \right)\\
&= \exp\left[ -\frac{1}{2}\left( \sum_{i=1}^n\frac{(x_i-\theta)^2}{\sigma_i^2} + \frac{(\theta - m)^2}{v}\right) \right]\\
&= \exp\left[ -\frac{1}{2}\left( \sum_{i=1}^n\frac{x_i^2}{\sigma_i^2} + \sum_{i=1}^n\frac{-2x_i\theta}{\sigma_i^2} + \sum_{i=1}^n\frac{\theta^2}{\sigma_i^2}  + \frac{\theta^2 - 2\theta m + m^2}{v} \right)\right]\\
&=\exp\left\lbrace -\frac{1}{2}\left[  \theta^2\left( \sum_{i=1}^n\frac{1}{\sigma_i^2} + \frac{1}{v} \right) -2 \theta\left(\sum_{i=1}^N\frac{x_i}{\sigma_i^2} + \frac{m}{v}\right) + \sum_{i=1}^n\frac{x_i^2}{\sigma_i^2} + \frac{m^2}{v} \right] \right\rbrace\\
\text{set }  &= \exp\left\lbrace -\frac{1}{2}\left[a(\theta^2 -2\theta b + b^2) \right] \right\rbrace\\
&= \exp\left\lbrace -\frac{1}{2}\left[\frac{(\theta-b)^2}{1/a} \right] \right\rbrace\\
\end{align*}

Matching the coefficients, we have:
\begin{align*}
a &= \sum_{i=1}^n\frac{1}{\sigma_i^2} + \frac{1}{v}\\
b &= \left(\sum_{i=1}^N\frac{x_i}{\sigma_i^2} + \frac{m}{v} \right) / \left( \sum_{i=1}^n\frac{1}{\sigma_i^2} + \frac{1}{v}\right)
\end{align*}

The posterior is:
\begin{align*}
\mbox{N}(b, 1/a)
\end{align*}

\subsection*{F}
\begin{align*}
p(x) &= \int_{0}^{\infty} p(x\mid \sigma^2) p(\sigma^2) d\sigma^2\\
&= \int_{0}^{\infty} p(x\mid \omega) p(\omega) d\omega\\
&= \int_{0}^{\infty} \left(\frac{\omega}{2\pi}\right)^{1/2}\exp\left(-\frac{\omega}{2}x^2\right)\frac{b^a}{\Gamma(a)}\omega^{a-1}\exp(-b\omega) d\omega\\
&\propto \int_{0}^{\infty} \omega^{1/2 + a - 1}\exp\left(-\omega\left(\frac{x^2}{2} + b\right)\right) d\omega\\
&= \frac{\Gamma(a+1/2)}{(b+x^2/2)^{a+1/2}}\\
&\propto \frac{\Gamma(\frac{2a+1}{2})}{(1 + \frac{x^2/2}{b})^{a+1/2}}
\end{align*}



\section{The multivariate normal distribution}
\subsection*{A}
\begin{align*}
\mbox{cov}(x) &= E\{(x - \mu)(x - \mu)^T\}\\
&= E\{xx^T - x\mu^T - \mu x^T + \mu\mu^T\}\\
&= E(xx^T) - E(x)\mu^T - \mu E(x)^T + \mu \mu^T\\
&=E(xx^T) - \mu \mu^T
\end{align*}

We have:
\begin{align*}
E(Ax + b) = A E(x) + b = A\mu + b
\end{align*}
then 
\begin{align*}
\mbox{cov}(Ax + b) &= E\{[(Ax+b) - (A\mu + b)][(Ax+b) - (A\mu + b)]^T\}\\
&= E\{ (Ax - A\mu)(Ax - A\mu)^T \}\\
&= E\{ A(x - \mu)(x - \mu)^TA^T \}\\
&= A E\{(x - \mu)(x - \mu)^T \} A^T\\
&= A \mbox{cov}(x)A^T
\end{align*}


\subsection*{B}

\begin{align*}
p(z) &= \prod_{i=1}^p p(z_i)\\
&= \frac{1}{(\sqrt{2\pi})^p}\exp\left(-\sum_{i=1}^p\frac{z_i^2}{2}\right)\\
&= \frac{1}{(\sqrt{2\pi})^p}\exp\left(-\frac{z^Tz}{2}\right)
\end{align*}

The MGF of \(z\) is:
\begin{align*}
E(\exp(t^Tz)) &= E\left[\exp\left(\sum_{i=1}^p t_iz_i\right)\right]\\
&= E\left[ \prod_{i=1}^p \exp(t_iz_i) \right]\\
&= \prod_{i=1}^p E[\exp(t_iz_i)]\\
&=\prod_{i=1}^p \exp(t_i^2/2)\\
&= \exp \left[ \sum_{i=1}^p t_i^2/2 \right]\\
&= \exp(t^Tt/2)
\end{align*}

\subsection*{C}


\subsection*{D}
We have \(z \sim \mbox{N}(0, \mbox{I})\) and \(z = Lz + \mu \).

\noindent
The MGF of \(x\) is:
\begin{align*}
E(\exp(t^Tx)) &= E[\exp(t^TLz + t^T\mu)]
\end{align*}
The expectation is with respect to \(z\), \(t^T\mu\) is a constant, we then look at:
\begin{align*}
E[\exp(t^TLz)] &= E\left[\exp\left(\sum_{i=1}^p \sum_{j=1}^p t_i L_{ij} z_j\right)\right]\\
&= E \left[ \prod_{j=1}^p \exp\left(\sum_{i=1}^p t_iL_{ij}z_j\right) \right]\\
&= \prod_{j=1}^p E \left[  \exp\left(\sum_{i=1}^p t_iL_{ij}z_j\right) \right] \tag{indepence}\\
&= \prod_{j=1}^p \mathrm{MGF}_{z_j}\left(\sum_{i=1}^p t_iL_{ij}\right) \\
&= \prod_{j=1}^p \exp\left(\frac{1}{2} (t^TL_j)^2 \right)\\
&= \prod_{j=1}^p \exp\left(\frac{1}{2} t^TL_jL_j^Tt \right)\\
&= \exp\left(\frac{1}{2}\sum_{j=1}^p t^TL_jL_j^Tt \right)\\
&= \exp\left(\frac{1}{2}t^TLL^Tt \right)\\
\end{align*}

\noindent
Come back to the MGF of \(x\):
\begin{align*}
E(\exp(t^Tx)) &= \exp \left(t^T\mu + \frac{1}{2}t^TLL^Tt \right)
\end{align*}

\noindent
Therefore, \(x \sim \mbox{N}(\mu, LL^T)\).
\end{document}