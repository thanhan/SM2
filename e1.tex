\documentclass{article}
\usepackage{amsmath}

\usepackage{listings}
\usepackage{color}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour}
    }

\lstset{style=mystyle}

\title{Statistical Modeling 2 \\ Exercise 1}

\begin{document}
\maketitle

\section{Bayesian inference in simple conjugate families}
\subsection*{A}
\begin{align}
p(w \mid x_1, \ldots, x_N) &\propto p(x_1, \ldots, x_N \mid w) p(w) \tag{Bayes rule} \\
&\propto \prod_{i=1}^N p(x_i|w) w^{a-1}(1-w)^{b-1} \tag{indepence} \\
&\propto w^s (1-w)^{N-s}w^{a-1}(1-w)^{b-1} \tag{let \(s = \sum_{i=1}^N x_i\)} \\
&= w^{s+a-1}(1-w)^{N-s+b-1} \tag*{}\\
& \propto \mathrm{Beta}(s+a, N-s+b) \tag*{}
\end{align}

\subsection*{B}
Let \(f(x_1, x_2) = (y1, y2) = (x_1/(x_1+x_2), x_1+x_2) \), we have:
\begin{align*}
f^{-1}(y1, y2) = (x_1, x_2) = (y_1y_2, y_2 - y_1y_2)
\end{align*}
We then calculate the Jacobian of \(f^{-1}\):
\begin{align*}
\partial x_1 / \partial y_1 &= y_2 \\
\partial x_1 / \partial y_2 &= y_1 \\
\partial x_2 / \partial y_1 &= -y_2 \\
\partial x_2 / \partial y_2 &= 1-y_1\\
\end{align*}
Therefore,
\begin{align*}
|J(f^{-1})| &= \begin{vmatrix}
y_2 & y_1 \\
-y_2 & 1 - y_1
\end{vmatrix} \\
&= y_2(1-y_1) + y_1y_2\\
&= y_2
\end{align*}
Let \(p_X\) be the joint density of \((x_1, x_2)\). We have the joint density of \(y_1\) and \(y_2\):
\begin{align*}
p(y_1, y_2) &= p_X(f^{-1}(y_1,y_2)) |J(f^{-1}(y_1, y_2))| \\
&=\mathrm{Ga}(y_1y_2; a_1, 1) \mathrm{Ga}(y2 - y_1y_2; a_2, 1) y_2\\
&= \frac{(y_1y_2)^{a_1-1} \exp(-y_1y_2)}{\Gamma(a_1)}
\frac{((1-y_1)y_2)^{a_2-1} \exp(y_1y_2-y_2)}{\Gamma(a_2)} y_2\\
&= \frac{y_1^{a_1-1}y_2^{a_1+a_2-1}(1-y_1)^{a_2-1}\exp(-y_2)}{\Gamma(a_1)\Gamma(a_2)}
\end{align*}
The marginals are:
\begin{align*}
p(y_1) &= \int_{0}^{\infty} p(y_1, y_2) dy_2\\
&= \frac{y_1^{a_1-1}(1-y_1)^{a_2-1}}{\Gamma(a_1)\Gamma(a_2)} \int_{0}^{\infty}y_2^{a_1+a_2-1}\exp(-y_2) dy_2\\
&=\frac{y_1^{a_1-1}(1-y_1)^{a_2-1}\Gamma
(a_1+a_2)}{\Gamma(a_1)\Gamma(a_2)}
\end{align*}
and 
\begin{align*}
p(y_2) &= \int_{0}^{\infty} p(y_1, y_2) dy_1\\
&=\frac{y_2^{a_1+a_2-1}\exp(-y_2)}{\Gamma(a_1)\Gamma(a_2)}\int_{0}^{\infty}y_1^{a_1-1}(1-y_1)^{a_2-1}\\
&= \frac{y_2^{a_1+a_2-1}\exp(-y_2)}{\Gamma(a_1)\Gamma(a_2)} \mathrm{Beta}(a_1, a_2)
\end{align*}

We can simulate a Beta random variable by taking two Gamma random variable as \(x_1\) and \(x_2\) and evaluate \(y_1\).

\subsection*{C}
The posterior is:
\begin{align*}
p(\theta| x_1, \ldots, x_N) &\propto p(x_1, \ldots, x_N | \theta) p(\theta)\\
&=\prod_{i=1}^N \mbox{N}(x_i; \theta, \sigma^2) \mbox{N}(\theta; m, v)\\
&\propto \prod_{i=1}^N \exp\left( -\frac{(x_i-\theta)^2}{2\sigma^2} \right) \exp\left( -\frac{(\theta - m)^2}{2v} \right)\\
&= \prod_{i=1}^N\exp\left(- \frac{x_i^2-2x_i\theta + \theta^2}{2\sigma^2} \right) \exp\left( -\frac{\theta^2 - 2\theta m + m^2}{2v} \right)\\
&=\exp\left( \frac{-\sum_i x_i^2+2\sum_i x_i\theta - N\theta^2}{2\sigma^2} \right) \exp\left( \frac{-\theta^2 + 2\theta m - m^2}{2v} \right)\\
&= \exp \left( -\theta^2\left(\frac{N}{2\sigma^2} + \frac{1}{2v} \right) + \theta\left(\frac{\sum_ix_i}{\sigma^2} + \frac{m}{v}\right) - \frac{\sum_ix_i^2}{2\sigma^2} - \frac{m^2}{2v} \right)
\end{align*}
We then complete the square by setting the posterior to:

\begin{align*}
&=\exp \left[ -a\left(\theta^2 -2b\theta + b^2\right) \right]\\
&=\exp \left[ -a\left(\theta - b\right)^2 \right]\\
&=\exp \left[ -\frac{(\theta - b)^2}{2(1/(2a))} \right]\\
\end{align*}


We calculate \(a, b\) by matching coefficients:

\begin{align*}
a &= \frac{N}{2\sigma^2} + \frac{1}{2v} = \frac{Nv + \sigma^2}{2\sigma^2v}\\
2ab &= \frac{\sum_ix_i}{\sigma^2} + \frac{m}{v}\\
\implies b &= \frac{v\sum_ix_i + m\sigma^2}{v\sigma^2} \frac{\sigma^2v}{Nv + \sigma^2}\\
&=\frac{v\sum_ix_i + m\sigma^2}{Nv + \sigma^2}
\end{align*}

The posterior is then:
\begin{align*}
&\mbox{N}(b, 1/(2a))\\
=&\mbox{N}\left(\frac{v\sum_ix_i + m\sigma^2}{Nv + \sigma^2}, \frac{\sigma^2v}{Nv + \sigma^2} \right)
\end{align*}

\subsection*{D}
\begin{align*}
p(\omega \mid x_1, \ldots, x_N) &\propto \prod_{i=1}^N p(x_i \mid \theta, \omega) p(\omega)\\
&\propto \prod_{i=1}^N \omega^{1/2} \exp \left[- \frac{\omega}{2}(x_i-\theta)^2 \right] \frac{b^a}{\Gamma(a)}\omega^{a-1}\exp(-b\omega)\\
&\propto \omega^{N/2 + a - 1} \exp\left[ -\omega \left( b + \frac{\sum_i(x_i-\theta)^2}{2} \right) \right]\\
&\propto \mbox{Ga}\left(a+\frac{N}{2}, b + \frac{\sum_i(x_i-\theta)^2}{2} \right)
\end{align*}
We have the posterior of the variance:
\begin{align*}
p(\sigma^2\mid x_1, \ldots, x_N) = \mbox{IG}\left(a+\frac{N}{2}, b + \frac{\sum_i(x_i-\theta)^2}{2} \right)
\end{align*}

\subsection*{E}
The posterior is:
\begin{align*}
p(\theta| x_1, \ldots, x_N) &\propto p(x_1, \ldots, x_N | \theta) p(\theta)\\
&=\prod_{i=1}^N \mbox{N}(x_i; \theta, \sigma_i^2) \mbox{N}(\theta; m, v)\\
&\propto \prod_{i=1}^N \exp\left( -\frac{(x_i-\theta)^2}{2\sigma_i^2} \right) \exp\left( -\frac{(\theta - m)^2}{2v} \right)\\
&= \exp\left( -\sum_{i=1}^n\frac{(x_i-\theta)^2}{2\sigma_i^2}  -\frac{(\theta - m)^2}{2v} \right)\\
&= \exp\left[ -\frac{1}{2}\left( \sum_{i=1}^n\frac{(x_i-\theta)^2}{\sigma_i^2} + \frac{(\theta - m)^2}{v}\right) \right]\\
&= \exp\left[ -\frac{1}{2}\left( \sum_{i=1}^n\frac{x_i^2}{\sigma_i^2} + \sum_{i=1}^n\frac{-2x_i\theta}{\sigma_i^2} + \sum_{i=1}^n\frac{\theta^2}{\sigma_i^2}  + \frac{\theta^2 - 2\theta m + m^2}{v} \right)\right]\\
&=\exp\left\lbrace -\frac{1}{2}\left[  \theta^2\left( \sum_{i=1}^n\frac{1}{\sigma_i^2} + \frac{1}{v} \right) -2 \theta\left(\sum_{i=1}^N\frac{x_i}{\sigma_i^2} + \frac{m}{v}\right) + \sum_{i=1}^n\frac{x_i^2}{\sigma_i^2} + \frac{m^2}{v} \right] \right\rbrace\\
\text{set }  &= \exp\left\lbrace -\frac{1}{2}\left[a(\theta^2 -2\theta b + b^2) \right] \right\rbrace\\
&= \exp\left\lbrace -\frac{1}{2}\left[\frac{(\theta-b)^2}{1/a} \right] \right\rbrace\\
\end{align*}

Matching the coefficients, we have:
\begin{align*}
a &= \sum_{i=1}^n\frac{1}{\sigma_i^2} + \frac{1}{v}\\
b &= \left(\sum_{i=1}^N\frac{x_i}{\sigma_i^2} + \frac{m}{v} \right) / \left( \sum_{i=1}^n\frac{1}{\sigma_i^2} + \frac{1}{v}\right)
\end{align*}

The posterior is:
\begin{align*}
\mbox{N}(b, 1/a)
\end{align*}

\subsection*{F}
\begin{align*}
p(x) &= \int_{0}^{\infty} p(x\mid \sigma^2) p(\sigma^2) d\sigma^2\\
&= \int_{0}^{\infty} p(x\mid \omega) p(\omega) d\omega\\
&= \int_{0}^{\infty} \left(\frac{\omega}{2\pi}\right)^{1/2}\exp\left(-\frac{\omega}{2}x^2\right)\frac{b^a}{\Gamma(a)}\omega^{a-1}\exp(-b\omega) d\omega\\
&= \frac{b^a}{(2\pi)^{1/2}\Gamma(a)} \int_{0}^{\infty} \omega^{1/2 + a - 1}\exp\left(-\omega\left(\frac{x^2}{2} + b\right)\right) d\omega\\
&= \frac{b^a}{(2\pi)^{1/2}\Gamma(a)} \frac{\Gamma(a+1/2)}{(b+x^2/2)^{a+1/2}} \tag{Gamma integral}\\
&= \frac{\Gamma(a+1/2)}{(2\pi b)^{1/2}\Gamma(a)(1+\frac{x^2}{2b})^{a+1/2}}\\
\end{align*}
Let \(\nu = 2a\) and \(\lambda = a/b\), we have:
\begin{align*}
p(x) = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)} \left(\frac{\lambda}{\pi \nu}\right)^{1/2} \left(1 + \frac{\lambda x^2}{2}\right)^{-\frac{\nu+1}{2}}
\end{align*}
This is the Student t distribution with \(\nu\) degree of freedom and `precision' \(\lambda\).

\section{The multivariate normal distribution}
\subsection*{A}
\begin{align*}
\mbox{cov}(x) &= E\{(x - \mu)(x - \mu)^T\}\\
&= E\{xx^T - x\mu^T - \mu x^T + \mu\mu^T\}\\
&= E(xx^T) - E(x)\mu^T - \mu E(x)^T + \mu \mu^T\\
&=E(xx^T) - \mu \mu^T
\end{align*}

We have:
\begin{align*}
E(Ax + b) = A E(x) + b = A\mu + b
\end{align*}
then 
\begin{align*}
\mbox{cov}(Ax + b) &= E\{[(Ax+b) - (A\mu + b)][(Ax+b) - (A\mu + b)]^T\}\\
&= E\{ (Ax - A\mu)(Ax - A\mu)^T \}\\
&= E\{ A(x - \mu)(x - \mu)^TA^T \}\\
&= A E\{(x - \mu)(x - \mu)^T \} A^T\\
&= A \mbox{cov}(x)A^T
\end{align*}


\subsection*{B}

\begin{align*}
p(z) &= \prod_{i=1}^p p(z_i)\\
&= \frac{1}{(\sqrt{2\pi})^p}\exp\left(-\sum_{i=1}^p\frac{z_i^2}{2}\right)\\
&= \frac{1}{(\sqrt{2\pi})^p}\exp\left(-\frac{z^Tz}{2}\right)
\end{align*}

The MGF of \(z\) is:
\begin{align*}
E(\exp(t^Tz)) &= E\left[\exp\left(\sum_{i=1}^p t_iz_i\right)\right]\\
&= E\left[ \prod_{i=1}^p \exp(t_iz_i) \right]\\
&= \prod_{i=1}^p E[\exp(t_iz_i)]\\
&=\prod_{i=1}^p \exp(t_i^2/2)\\
&= \exp \left[ \sum_{i=1}^p t_i^2/2 \right]\\
&= \exp(t^Tt/2)
\end{align*}

\subsection*{C}
We need to prove that for all vector \(a\) not identically zero, the scalar quantity \(z = a^T x\) is normally distributed if and only if
\begin{align*}
E[\exp(t^Tx)] = \exp(t^T\mu + t^T\Sigma t/2)
\end{align*}

\noindent
{\bf (only if)} We have that \(z = a^T x\) is normally distributed:
\begin{align*}
\mathrm{MGF}_z(s) = E[\exp(sa^Tx)] = \exp(ms + vs^2/2)
\end{align*}
Consider:



\subsection*{D}
We have \(z \sim \mbox{N}(0, \mbox{I})\) and \(x = Lz + \mu \).

\noindent
The MGF of \(x\) is:
\begin{align*}
E(\exp(t^Tx)) &= E[\exp(t^TLz + t^T\mu)]
\end{align*}
The expectation is with respect to \(z\), \(t^T\mu\) is a constant, we then look at:
\begin{align*}
E[\exp(t^TLz)] &= E\left[\exp\left(\sum_{i=1}^p \sum_{j=1}^p t_i L_{ij} z_j\right)\right]\\
&= E \left[ \prod_{j=1}^p \exp\left(\sum_{i=1}^p t_iL_{ij}z_j\right) \right]\\
&= \prod_{j=1}^p E \left[  \exp\left(\sum_{i=1}^p t_iL_{ij}z_j\right) \right] \tag{indepence}\\
&= \prod_{j=1}^p \mathrm{MGF}_{z_j}\left(\sum_{i=1}^p t_iL_{ij}\right) \\
&= \prod_{j=1}^p \exp\left(\frac{1}{2} (t^TL_j)^2 \right)\\
&= \prod_{j=1}^p \exp\left(\frac{1}{2} t^TL_jL_j^Tt \right)\\
&= \exp\left(\frac{1}{2}\sum_{j=1}^p t^TL_jL_j^Tt \right)\\
&= \exp\left(\frac{1}{2}t^TLL^Tt \right)\\
\end{align*}

\noindent
Come back to the MGF of \(x\):
\begin{align*}
E(\exp(t^Tx)) &= \exp \left(t^T\mu + \frac{1}{2}t^TLL^Tt \right)
\end{align*}

\noindent
Therefore, \(x \sim \mbox{N}(\mu, LL^T)\).

\subsection*{E}
We have that \(x\) has a multivariate normal distribution: \(x \sim \mbox{N}(\mu, \Sigma)\). The covariance matrix \(\Sigma\) is symmetric positive definite and has a Cholesky decomposition:
\begin{align*}
\Sigma = L L^T
\end{align*}
where \(L\) is a lower triangular matrix with positive diagonal entries and therefore invertible. Let
\begin{align*}
z = L^{-1}(x-\mu)
\end{align*}

Consider the MGF of \(z\):
\begin{align*}
E[\exp(t^Tz)] &= E[\exp(t^TL^{-1}(x-\mu))]\\
&= E[\exp(t^TL^{-1}x) \cdot \exp(-t^TL^{-1}\mu)]\\
&= \mathrm{MGF}_x(t^TL^{-1}) \cdot \exp(-t^TL^{-1}\mu)\\
&= \exp(t^TL^{-1}\mu + t^TL^{-1}\Sigma L^{-T}t/2) \cdot \exp(-t^TL^{-1}\mu)\\
&= \exp(t^TL^{-1}\Sigma L^{-T}t/2)\\
&= \exp(t^TL^{-1} (LL^T) L^{-T}t/2)\\
&= \exp(t^Tt/2)\\
\end{align*}
We conclude that \(z\) has standard multivariate normal distribution and that \(x\) can be written as an affine transformation of standard normal distribution.

\subsection*{F}
Let \(z\) be standard multivariate Normal:
\begin{align*}
p_Z(z) \propto \exp\left(-\frac{z^tz}{2}\right)
\end{align*}

By the previous result, we have that \(x = Lz + \mu\) has multivariate Normal distribution. Since \(L\) is full rank, it is invertible, let \(z = f(x) = L^{-1}(x - \mu)\)

The pdf of \(x\) is:
\begin{align*}
p_X(x) &= p_Z(f(x)) |J_f(x)|\\
& \propto \exp\left(-\frac{(x-\mu)^TL^{-T} L^{-1}(x-\mu)}{2}\right) |L^{-1}|\\
& \propto \exp( -Q(x-\mu)/2)
\end{align*}


\subsection*{G}
By the previous results, \(x_1\) and \(x_2\) are affine transformation of independent standard Normal distribution. Let \(z \sim \mbox{N}(0, I)\)
\begin{align*}
x_1 &= L_1z + \mu_1\\
x_2 &= L_2z + \mu_2\\
\end{align*}
We have:
\begin{align*}
y = Ax_1 + Bx_2 &= AL_1z +  A\mu_1 + BL_2z + B\mu_2\\
&= (AL_1 + BL_2)z +  A\mu_1 + B\mu_2\\
\end{align*}

We see that \(y\) is an affine transformation of independent standard Normal variables and therefore is multivariate Normal with mean \(A\mu_1 + B\mu_2\) and variance
\begin{align*}
(AL_1 + BL_2)(AL_1 + BL_2)^T &= AL_1L_1^TA^T + AL_1L_2^TB^T + BL_2L_1^TA^T + BL_2L_2^TB^T\\
&= A \Sigma_1 A^T + AL_1L_2^TB^T + BL_2L_1^TA^T + B\Sigma_2B^T
\end{align*}
\section{Conditionals and marginals}
\subsection*{A}

Decompose the covariance matrix \(\Sigma\) and partition \(L\) into \(L_1\) and \(L_2\) where \(L_1\) has \(k\) elements and corresponds to \(x_1\).


\begin{align*}
\Sigma &= LL^T\\
&= \begin{pmatrix}
L_1\\
L_2
\end{pmatrix}\begin{pmatrix}
L_1^T & L_2^T
\end{pmatrix}\\
&= \begin{pmatrix}
L_1L_1^T & L_1L_2^T\\
L_2L_1^T & L_2L_2^T
\end{pmatrix}\\
\end{align*}

We have that \(\Sigma_{11} = L_1L_1^T\). By the previous results, \(x = Lz + \mu\) where \(z\) is a vector of independent standard Normal variables. Take the first \(k\) element, we have:

\begin{align*}
x_1 = L_1z_1 + \mu_1\\
\end{align*}
where \(z_1\) is also a vector of independent standard Normal variables. Therefore, \(x_1\) also has multivariate Normal distribution with mean \(mu_1\) and variance \(L_1L_1^T = \Sigma_{11}\).





\subsection*{B}
\begin{align*}
\begin{pmatrix}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix}^{-1} &= \begin{pmatrix}
(\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1}  & -\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}  \\
-\Sigma_{22}^{-1}\Sigma_{21}(\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1} & (\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}
\end{pmatrix}\\
&= \begin{pmatrix}
\Omega_{11} & \Omega_{12}\\
\Omega_{12}^T & \Omega{22}
\end{pmatrix}
\end{align*}
\subsection*{C}
\begin{align*}
\log p(x_1|x_2) &= \log p(x_1, x_2) - \log(x_2)\\
&= \mbox{const} -\frac{(x-\mu)^T\Sigma^{-1}(x-\mu)}{2}\\
&=\mbox{const} - \frac{1}{2} \left\lbrace \begin{pmatrix} x_1 - \mu_1 \\x_2 - \mu_2 \end{pmatrix}^{T} \begin{pmatrix}
\Omega_{11} & \Omega_{12}\\
\Omega_{12}^T & \Omega{22}
\end{pmatrix} \begin{pmatrix} x_1 - \mu_1 \\x_2 - \mu_2 \end{pmatrix}   \right\rbrace\\
&= \mbox{const} - \frac{1}{2} \left\lbrace (x_1 - \mu_1)^T\Omega_{11}(x_1-\mu_1) + (x_2-\mu_2)^T\Omega_{12}^T(x_1-\mu_1) \right.  \\
& \left.  + (x_1 - \mu_1)^T\Omega_{12}(x_2-\mu_2) + (x_2-\mu_2)^T\Omega_{22}^T(x_2-\mu_2) \right\rbrace\\
&= \mbox{const} - \frac{1}{2} \left\lbrace (x_1 - \mu_1)^T\Omega_{11}(x_1-\mu_1) + 2(x_1-\mu_1)^T\Omega_{12}(x_2-\mu_2) \right\rbrace\\
&= \mbox{const} - \frac{1}{2} \left\lbrace x_1^T\Omega_{11}x_1 - x_1^T\Omega_{11}\mu_1 - \mu_1^T\Omega_{11}x_1 +   2x_1^T\Omega_{12}x_2 - 2x_1^T\Omega_{12}\mu_2 \right\rbrace\\
&= \mbox{const} - \frac{1}{2}\left\lbrace x_1^T\Omega_{11}x_1 + x_1^T\left(  - \Omega_{11}\mu_1 - \Omega_{11}^T\mu_1 + 2\Omega_{12}x_2 - 2\Omega_{12}\mu_2  \right)   \right\rbrace\\
&= \mbox{const} - \frac{1}{2}\left\lbrace x_1^T\Omega_{11}x_1 - 2 x_1^T\Omega_{11}\left(  \mu_1 - \Omega_{11}^{-1}\Omega_{12}x_2  + \Omega_{11}^{-1}\Omega_{12}\mu_2  \right)   \right\rbrace\\
&= \mbox{const} - \frac{1}{2}\left\lbrace (x_1 - \mu_{1|2})^T\Omega_{11}(x_1 - \mu_{1|2})  \right\rbrace\\
\end{align*}
where:
\begin{align*}
\mu_{1|2} &= \mu_1 - \Omega_{11}^{-1}\Omega_{12}x_2  + \Omega_{11}^{-1}\Omega_{12}\mu_2\\
\end{align*}
We also have:
\begin{align*}
\Omega_{11}^{-1}\Omega_{12} &=  (\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T) (-\Sigma_{11}^{-1})\Sigma_{12}(\Sigma_{22} - \Sigma_{12}^T \Sigma_{11}^{-1} \Sigma_{12})^{-1}\\
&= (-\Sigma_{12} + \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T\Sigma_{11}^{-1}\Sigma_{12})(\Sigma_{22} - \Sigma_{12}^T \Sigma_{11}^{-1} \Sigma_{12})^{-1}\\
&= (-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22} + \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T\Sigma_{11}^{-1}\Sigma_{12})(\Sigma_{22} - \Sigma_{12}^T \Sigma_{11}^{-1} \Sigma_{12})^{-1}\\
&= -\Sigma_{12}\Sigma_{22}^{-1}(\Sigma_{22} - \Sigma_{12}^T\Sigma_{11}^{-1}\Sigma_{12})(\Sigma_{22} - \Sigma_{12}^T \Sigma_{11}^{-1} \Sigma_{12})^{-1}\\
&= -\Sigma_{12}\Sigma_{22}^{-1}
\end{align*}
Therefore,
\begin{align*}
\mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2)
\end{align*}

We conclude that \(p(x_1|x_2)\) has Normal distribution with mean \(\mu_{1|2}\) given above and variance \(\Omega_{11}^{-1} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T\)


\section{Multiple regression: three classical principles for inference}

\subsection{A}
In the least square estimate, we minimize:
\begin{align*}
L(\beta) &= \sum_{i=1}^n (y_i - x_i^T\beta)^2\\
&= (y - X\beta)^T(y - X\beta)\\
&= -y^T y -y^TX\beta - \beta^TX^T y + \beta^TX^T X\beta\\
\end{align*}
where we define \(y = (y_1, \ldots, y_n)^T\) as a \(n\times 1\) vector and \(X = (x_1^T; \ldots; x_n^T)\) as a \(n\times p\) matrix.
We take the derivative of \(L\) and set to \(0\):
\begin{align*}
\frac{\partial L}{\partial \beta} &= - X^T y - X^T y + X^T X \beta + X^T X \beta\\
&= 2 X^T (X \beta - y) \\
& \text{  set } = 0 \\
\implies & X^TX\beta = X^Ty\\
&\beta = (X^TX)^{-1}X^Ty
\end{align*}

In the maximum likelihood estimate, we maximize:
\begin{align*}
L(\beta) &= \prod_{i=1}^n p(y_i \mid \beta, \sigma^2)\\
&=   N(y \mid X\beta, \sigma^2 I)\\
&\propto \exp \left( -\frac{1}{2} (y-X\beta)^T \frac{1}{\sigma^2}I (y-X\beta) \right)
\end{align*}
%which is equivalent to maximize the log likelihood:
%\begin{align*}
%l(\beta) &= \sum_{i=1}^n \log p(y_i \mid \beta, \sigma^2)\\
%&= \sum_{i=1}^n \log N(y_i \mid \beta, \sigma^2)\\
%\end{align*}
which is equivalent to minimize:
\begin{align*}
&(y-X\beta)^T \frac{1}{\sigma^2}I (y-X\beta)\\
&\propto (y-X\beta)^T(y-X\beta)
\end{align*}
which is the same as the least square objective function.

In the method of moment estimate, we set:
\begin{align*}
\mathrm{cov}(y - X\beta, X_j) = 0
\end{align*}
where \(X_j\) is the column \(j\) of \(X\) for \(j = 1, \ldots , p\). We have:
\begin{align*}
&\mathrm{cov}(y - X\beta, X_j) = 0 \hspace{0.5cm} \forall j\\
\iff &(y - X\beta)^T X_j = 0 \hspace{0.5cm} \forall j\\
\iff & (y - X\beta)^T X = 0 \\
\iff & X^T(X\beta - y) = 0 \\
\end{align*}
This is the same as the equation that we solve in least square.

\subsection*{B}
The weighted sum of squared error is:
\begin{align*}
L(\beta) &= \sum_{i=1}^n w_i (y_i - x_i^T\beta)^2\\
\end{align*}
We first take the derivative with respect to \(\beta\):
\begin{align*}
\frac{\partial L}{\partial \beta} = \sum_{i=1}^n w_i (2) (y_i - x_i^T\beta) (-x_i)
\end{align*}
Setting the derivative to \(0\), we have:
\begin{align*}
\sum_{i=1}^n w_i x_i (y_i - x_i^T\beta) = 0\\
\iff \sum_{i=1}^n w_i x_i y_i - \sum_{i=1}^n w_i x_i x_i^T\beta = 0\\
\iff \sum_{i=1}^n w_i x_{ik} y_i - \sum_{i=1}^n w_i x_{ik} x_{i}^T\beta_k = 0 \hspace{0.5cm} \forall k = 1, \ldots, p \\
\iff \beta_k = \frac{\sum_{i=1}^n w_i x_{ik} y_i}{\sum_{i=1}^n w_i x_{ik} x_{i}^T\beta_k} \hspace{0.5cm} \forall k = 1, \ldots, p \\
\end{align*}

The maximum likelihood under heteroskedastic Gaussian error is:
\begin{align*}
L(\beta) &= \prod_{i=1}^n p(y_i \mid \beta, \sigma_i^2)\\
&= N(y \mid X\beta, C)\\
\end{align*}
where the covariance matrix is \(C\) is a diagonal matrix such that \(C_{ii} = \sigma^2_i\). We have that \(C^{-1} = D\) is a diagonal matrix such that \(D_{ii} = 1/\sigma^2_i\).
The maximum likelihood is then:
\begin{align*}
L(\beta) = N(y \mid X\beta, C) \propto \exp\left(-\frac{1}{2}(y - X\beta)^TD(y - X\beta)\right)\\
\end{align*}
Maximizing the that likelihood is the same as minimizing:
\begin{align*}
(y - X\beta)^TD(y - X\beta) &= \sum_{i=1}^n (y_i - x_i^T\beta)\frac{1}{\sigma^2}(y_i - x_i^T\beta)\\
&= \sum_{i=1}^n w_i (y_i - x_i^T\beta)^2
\end{align*}
where \(w_i = \frac{1}{\sigma_i^2}\).

This is the same objective as the weighted least square.

\section{Quantifying uncertainty: some basic frequentist ideas}
\textit{In linear regression}
\subsection*{A}

The estimate is:
\begin{align*}
\hat{\beta} = (X^TX)^{-1}X^Ty
\end{align*}
which is a linear combination of \(y\). Since \(y\) has Normal distribution, \(\hat{\beta}\) also has Normal distribution.
\begin{align*}
E(\hat{\beta}) &= (X^TX)^{-1}X^T E(y)\\
&= (X^TX)^{-1}X^T X\beta\\
&= \beta
\end{align*}

\begin{align*}
\mbox{cov}(\hat{\beta}) &= (X^TX)^{-1}X^T \mbox{cov}(x) X(X^TX)^{-T}\\
&= (X^TX)^{-1}X^T (\sigma^2 I) X(X^TX)^{-T}\\
&= \sigma^2 (X^TX)^{-1}X^T  X(X^TX)^{-T}\\
&= \sigma^2 (X^TX)^{-T}
\end{align*}

\subsection*{B}
We can estimate \(\sigma^2\) by:
\begin{align*}
\hat{\sigma^2} = \frac{\sum_{i=1}^n\hat{\epsilon}_i^2}{n-p} = \frac{\sum_{i=1}^n(x_i\hat{\beta} - y_i)^2}{n-p} = \frac{(X\hat{\beta} - y)^T(X\hat{\beta} - y)}{n-p}
\end{align*}

\begin{lstlisting}[language=R, caption = ]
# Load the library
# you might have to install this the first time
library(mlbench)

# Load the data
ozone = data(Ozone, package='mlbench')

# Look at the help file for details
?Ozone

# Scrub the missing values
# Extract the relevant columns 
ozone = na.omit(Ozone)[,4:13]

y = ozone[,1]
x = as.matrix(ozone[,2:10])

# add an intercept
x = cbind(1,x)

# compute the estimator
betahat = solve(t(x) %*% x) %*% t(x) %*% y

# Fill in the blank
sshat = t(x %*% betahat - y) %*% (x %*% betahat - y)
sshat = sshat[1,1] / (dim(x)[1] - dim(x)[2])
betacov = sshat * t(solve(t(x) %*% x) )

# Now compare to lm
# the 'minus 1' notation says not to fit an intercept
#(we've already hard-coded it as an extra column)
lm1 = lm(y~x-1)

summary(lm1)
betacovlm = vcov(lm1)
sqrt(diag(betacovlm))




\end{lstlisting}


\textit{Propagating uncertainty}
\subsection*{A}
\begin{align*}
\mbox{var}(f) &= \mbox{var}(\theta_1 + \theta_2)\\
&= \mbox{var}(\theta_1) + \mbox{var}(\theta_2) + 2\mbox{cov}(\theta_1, \theta_2)\\
&= \hat{\Sigma}_{11} + \hat{\Sigma}_{22} + 2\hat{\Sigma}_{12}\\
\end{align*}
In the general case, when \(f\) is the sum of \(p\) components of \(\theta\), we have:
\begin{align*}
\mbox{var}(f) &= \mbox{var}(\theta_1 + \ldots + \theta_p)\\
&= \mbox{var}(\theta_1) + \dots + \mbox{var}(\theta_p) + \sum_{i\neq j}\mbox{cov}(\theta_i, \theta_j)\\
&= \sum_{i,j = 1}^p \hat{\Sigma}_{ij}
\end{align*}

\subsection*{B}
\end{document}